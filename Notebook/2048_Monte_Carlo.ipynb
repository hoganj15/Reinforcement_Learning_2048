{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2048 Monte Carlo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F66v_C1towTt",
        "outputId": "9eb84e1d-c4d7-4d57-edff-9d522e652014"
      },
      "source": [
        "!pip install gym-2048"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-2048\n",
            "  Downloading https://files.pythonhosted.org/packages/47/47/a1f06f3a6a05b78ffee842a7cd6f6734f97b0c3600f69df7654c232f0adc/gym-2048-0.2.6.tar.gz\n",
            "Collecting gym~=0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/04/70d4901b7105082c9742acd64728342f6da7cd471572fd0660a73f9cfe27/gym-0.10.11.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 3.5MB/s \n",
            "\u001b[?25hCollecting numpy~=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/84/49b7f268741119328aeee0802aafb9bc2e164b36fc312daf83af95dae646/numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 288kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2020.12.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym~=0.10.0->gym-2048) (0.16.0)\n",
            "Building wheels for collected packages: gym-2048, gym\n",
            "  Building wheel for gym-2048 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-2048: filename=gym_2048-0.2.6-cp37-none-any.whl size=4699 sha256=dd1f843760d4eaa537d14419927c8f7c8b06857224b45510c22b321b830616e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/11/22/a6c0fb3622f6fad7e8b7f4342a3c64e115bee08a4189bbae18\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-cp37-none-any.whl size=1588312 sha256=3cbd0119f5c433fb8d80ed4facaea004fc260949387b184dd75d22a30cdc73da\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/eb/1f/22c4124f3c64943aa0646daf4612b1c1f00f27d89b81304ebd\n",
            "Successfully built gym-2048 gym\n",
            "\u001b[31mERROR: xarray 0.18.2 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2021.4.8 has requirement numpy>=1.15.1, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pymc3 3.11.2 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 2.0.0 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.5 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.51.2 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jaxlib 0.1.66+cuda110 has requirement numpy>=1.16, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, gym, gym-2048\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.10.11 gym-2048-0.2.6 numpy-1.14.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21HCytJeoa58"
      },
      "source": [
        "import gym\n",
        "import gym_2048\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwnOq4bBhC3k"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvdbc1KEaMGQ"
      },
      "source": [
        "Custom class to be able to define the inital starting state - going to be used for Monte Carlo tree search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPoPeVG2Zn__"
      },
      "source": [
        "class NewBase2048Env(gym_2048.Base2048Env):\n",
        "  def __init__(self, *args):\n",
        "    gym_2048.Base2048Env.__init__(self, *args)\n",
        "  def custom_reset(self, starting_board): #new function to allow reset with initial conditions\n",
        "    self.board = starting_board\n",
        "    return self.board"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyAVxxV-paBy"
      },
      "source": [
        "# Monte Carlo Tree Search Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AD8YkrwnNwb"
      },
      "source": [
        "This function takes two arguments:\n",
        "\n",
        "- `run_simulations`: The number of move sets the algorithm should simulate before making each real move\n",
        "- `move_simulations`: The number of moves it should simulate in each run\n",
        "\n",
        "For instance, if `run_simulations` is 50 and `move_simulations` is 10, then before each real move, the algorithm will simulate 50 random sets of 10 moves. Increasing these values will improve performance but increase the time to compute as well.\n",
        "\n",
        "The algorithm will track the total score accumulated during each simulated run, and will choose the next move to be the one which had the highest average score for all of its simulated runs. For instance, in the example above, the algorithm will test 50 simulated runs of 10 random moves. For each possible move (up, down, left, right), the algorithm will find the average total score accumulated for all random runs which began with that move. Then, it will choose the next move to be the one which had the highest average score accumulated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcyNvWzwke8Q"
      },
      "source": [
        "import time\n",
        "\n",
        "def monte_carlo_2048(run_simulations=50, move_simulations=10, criterion='avg'):\n",
        "  start = time.time()\n",
        "  env = NewBase2048Env()\n",
        "\n",
        "  obs, reward, done, _ = env.step(2)\n",
        "  moves_made = 0\n",
        "  total_score = 0\n",
        "  while not done:\n",
        "    current_board = env.board.copy() #making a copy of the current board\n",
        "\n",
        "    future_moves = {0:[], 1:[], 2:[], 3:[]} #list of all ending scores for each possible next move\n",
        "    for simulated_run in range(run_simulations):\n",
        "      reward = 0\n",
        "\n",
        "      simulated_env = NewBase2048Env()\n",
        "      simulated_env.custom_reset(current_board) #initiate a simulated environment with the current board setup\n",
        "\n",
        "      action = np.random.choice([0, 1, 2, 3]) #first action\n",
        "      _, score, lost, _ = simulated_env.step(action)\n",
        "      reward += score #add the number of points from the move to the total reward from this run\n",
        "      if lost: #if the game is done, set reward to 0 and break the simulated run\n",
        "        reward = 0\n",
        "        \n",
        "      for simulated_move in range(move_simulations): #additional random actions\n",
        "        _, score, lost, _ = simulated_env.step(np.random.choice([0, 1, 2, 3]))\n",
        "        reward += score\n",
        "        if lost:\n",
        "          break\n",
        "          \n",
        "      future_moves[action].append(reward) #append the total points accumulated from this run to the future_moves dictionary\n",
        "    \n",
        "    if criterion=='max':\n",
        "      #look at max rewards for each possible next move, and choose the one with highest max reward\n",
        "      best_reward = 0\n",
        "      best_move = 0\n",
        "      for key in future_moves:\n",
        "        if np.max(future_moves[key]) > best_reward:\n",
        "          best_reward = np.max(future_moves[key])\n",
        "          best_move = key\n",
        "    else:\n",
        "      #look at average rewards for each possible next move, and choose the one with highest average\n",
        "      best_reward = 0\n",
        "      best_move = 0\n",
        "      for key in future_moves:\n",
        "        if np.mean(future_moves[key]) > best_reward:\n",
        "          best_reward = np.mean(future_moves[key])\n",
        "          best_move = key\n",
        "    \n",
        "    obs, reward, done, _ = env.step(best_move)\n",
        "    total_score += reward\n",
        "    moves_made += 1\n",
        "    if moves_made % 50 == 0:\n",
        "      print(f'{moves_made} moves made')\n",
        "      print(f'Total Score: {total_score}')\n",
        "      print(f'Current best tile: {np.max(env.board)}')\n",
        "      print()\n",
        "\n",
        "      \n",
        "  print(f'Final Score: {total_score}')\n",
        "  print(f'Time to train: {time.time()-start}')\n",
        "  print(f'Best tile achieved: {np.max(env.board)}')\n",
        "  print()\n",
        "  env.render()\n",
        "\n",
        "  return time.time()-start, np.max(env.board), total_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHsdhRuGsybd",
        "outputId": "67a375c2-e8e4-41b7-b5b7-522427bbd506"
      },
      "source": [
        "monte_carlo_2048(run_simulations=100, move_simulations=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 moves made\n",
            "Total Score: 444\n",
            "Current best tile: 64\n",
            "\n",
            "100 moves made\n",
            "Total Score: 1088\n",
            "Current best tile: 128\n",
            "\n",
            "150 moves made\n",
            "Total Score: 1968\n",
            "Current best tile: 256\n",
            "\n",
            "200 moves made\n",
            "Total Score: 2620\n",
            "Current best tile: 256\n",
            "\n",
            "250 moves made\n",
            "Total Score: 3548\n",
            "Current best tile: 256\n",
            "\n",
            "300 moves made\n",
            "Total Score: 4536\n",
            "Current best tile: 512\n",
            "\n",
            "350 moves made\n",
            "Total Score: 5248\n",
            "Current best tile: 512\n",
            "\n",
            "400 moves made\n",
            "Total Score: 6152\n",
            "Current best tile: 512\n",
            "\n",
            "450 moves made\n",
            "Total Score: 6744\n",
            "Current best tile: 512\n",
            "\n",
            "500 moves made\n",
            "Total Score: 9296\n",
            "Current best tile: 1024\n",
            "\n",
            "550 moves made\n",
            "Total Score: 9956\n",
            "Current best tile: 1024\n",
            "\n",
            "600 moves made\n",
            "Total Score: 10908\n",
            "Current best tile: 1024\n",
            "\n",
            "650 moves made\n",
            "Total Score: 11560\n",
            "Current best tile: 1024\n",
            "\n",
            "700 moves made\n",
            "Total Score: 12032\n",
            "Current best tile: 1024\n",
            "\n",
            "750 moves made\n",
            "Total Score: 13484\n",
            "Current best tile: 1024\n",
            "\n",
            "800 moves made\n",
            "Total Score: 14164\n",
            "Current best tile: 1024\n",
            "\n",
            "850 moves made\n",
            "Total Score: 15104\n",
            "Current best tile: 1024\n",
            "\n",
            "900 moves made\n",
            "Total Score: 15756\n",
            "Current best tile: 1024\n",
            "\n",
            "950 moves made\n",
            "Total Score: 20252\n",
            "Current best tile: 2048\n",
            "\n",
            "1000 moves made\n",
            "Total Score: 20916\n",
            "Current best tile: 2048\n",
            "\n",
            "1050 moves made\n",
            "Total Score: 21468\n",
            "Current best tile: 2048\n",
            "\n",
            "1100 moves made\n",
            "Total Score: 22312\n",
            "Current best tile: 2048\n",
            "\n",
            "1150 moves made\n",
            "Total Score: 22996\n",
            "Current best tile: 2048\n",
            "\n",
            "1200 moves made\n",
            "Total Score: 24436\n",
            "Current best tile: 2048\n",
            "\n",
            "1250 moves made\n",
            "Total Score: 25036\n",
            "Current best tile: 2048\n",
            "\n",
            "1300 moves made\n",
            "Total Score: 25980\n",
            "Current best tile: 2048\n",
            "\n",
            "1350 moves made\n",
            "Total Score: 26452\n",
            "Current best tile: 2048\n",
            "\n",
            "1400 moves made\n",
            "Total Score: 27124\n",
            "Current best tile: 2048\n",
            "\n",
            "1450 moves made\n",
            "Total Score: 29616\n",
            "Current best tile: 2048\n",
            "\n",
            "1500 moves made\n",
            "Total Score: 30260\n",
            "Current best tile: 2048\n",
            "\n",
            "1550 moves made\n",
            "Total Score: 31120\n",
            "Current best tile: 2048\n",
            "\n",
            "1600 moves made\n",
            "Total Score: 31804\n",
            "Current best tile: 2048\n",
            "\n",
            "1650 moves made\n",
            "Total Score: 32316\n",
            "Current best tile: 2048\n",
            "\n",
            "1700 moves made\n",
            "Total Score: 33752\n",
            "Current best tile: 2048\n",
            "\n",
            "1750 moves made\n",
            "Total Score: 34384\n",
            "Current best tile: 2048\n",
            "\n",
            "1800 moves made\n",
            "Total Score: 35300\n",
            "Current best tile: 2048\n",
            "\n",
            "Final Score: 35828\n",
            "Time to train: 1119.4155995845795\n",
            "Best tile achieved: 2048\n",
            "\n",
            "2 \t4 \t16 \t8\n",
            "8 \t2048 \t256 \t16\n",
            "16 \t128 \t512 \t4\n",
            "4 \t1024 \t8 \t2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1119.4169249534607, 2048, 35828)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD9iEMlELzZF"
      },
      "source": [
        "Running a fixed number of simulations for each possible next move instead of totally random moves **SLIGHT MODIFICATION OF ABOVE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcsokxMnoign"
      },
      "source": [
        "import time\n",
        "\n",
        "def monte_carlo_2048(run_simulations=50, move_simulations=10, criterion='avg'):\n",
        "  start = time.time()\n",
        "  env = NewBase2048Env()\n",
        "\n",
        "  obs, reward, done, _ = env.step(2)\n",
        "  moves_made = 0\n",
        "  total_score = 0\n",
        "  while not done:\n",
        "    current_board = env.board.copy() #making a copy of the current board\n",
        "\n",
        "    future_moves = {0:[], 1:[], 2:[], 3:[]} #list of all ending scores for each possible next move\n",
        "    for action in [0, 1, 2, 3]:\n",
        "      for simulated_run in range(run_simulations):\n",
        "        reward = 0\n",
        "\n",
        "        simulated_env = NewBase2048Env()\n",
        "        simulated_env.custom_reset(current_board) #initiate a simulated environment with the current board setup\n",
        "\n",
        "        _, score, lost, _ = simulated_env.step(action)\n",
        "        reward += score #add the number of points from the move to the total reward from this run\n",
        "        if lost: #if the game is done, set reward to 0 and break the simulated run\n",
        "          reward = 0\n",
        "          \n",
        "        for simulated_move in range(move_simulations): #additional random actions\n",
        "          _, score, lost, _ = simulated_env.step(np.random.choice([0, 1, 2, 3]))\n",
        "          reward += score\n",
        "          if lost:\n",
        "            break\n",
        "            \n",
        "        future_moves[action].append(reward) #append the total points accumulated from this run to the future_moves dictionary\n",
        "    \n",
        "    if criterion=='max':\n",
        "      #look at max rewards for each possible next move, and choose the one with highest max reward\n",
        "      best_reward = 0\n",
        "      best_move = 0\n",
        "      for key in future_moves:\n",
        "        if np.max(future_moves[key]) > best_reward:\n",
        "          best_reward = np.max(future_moves[key])\n",
        "          best_move = key\n",
        "    else:\n",
        "      #look at average rewards for each possible next move, and choose the one with highest average\n",
        "      best_reward = 0\n",
        "      best_move = 0\n",
        "      for key in future_moves:\n",
        "        if np.mean(future_moves[key]) > best_reward:\n",
        "          best_reward = np.mean(future_moves[key])\n",
        "          best_move = key\n",
        "    \n",
        "    obs, reward, done, _ = env.step(best_move)\n",
        "    total_score += reward\n",
        "    moves_made += 1\n",
        "    if moves_made % 50 == 0:\n",
        "      print(f'{moves_made} moves made')\n",
        "      print(f'Total Score: {total_score}')\n",
        "      print(f'Current best tile: {np.max(env.board)}')\n",
        "      print()\n",
        "\n",
        "      \n",
        "  print(f'Final Score: {total_score}')\n",
        "  print(f'Time to train: {time.time()-start}')\n",
        "  print(f'Best tile achieved: {np.max(env.board)}')\n",
        "  print()\n",
        "  env.render()\n",
        "\n",
        "  return time.time()-start, np.max(env.board), total_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0axvot-Gy6M",
        "outputId": "462416f6-0d50-4f1f-cee7-6605c14e5f0c"
      },
      "source": [
        "monte_carlo_2048()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 moves made\n",
            "Total Score: 448\n",
            "Current best tile: 64\n",
            "\n",
            "100 moves made\n",
            "Total Score: 1116\n",
            "Current best tile: 128\n",
            "\n",
            "150 moves made\n",
            "Total Score: 1968\n",
            "Current best tile: 256\n",
            "\n",
            "200 moves made\n",
            "Total Score: 2672\n",
            "Current best tile: 256\n",
            "\n",
            "250 moves made\n",
            "Total Score: 4128\n",
            "Current best tile: 512\n",
            "\n",
            "300 moves made\n",
            "Total Score: 4796\n",
            "Current best tile: 512\n",
            "\n",
            "350 moves made\n",
            "Total Score: 5292\n",
            "Current best tile: 512\n",
            "\n",
            "400 moves made\n",
            "Total Score: 6168\n",
            "Current best tile: 512\n",
            "\n",
            "450 moves made\n",
            "Total Score: 6884\n",
            "Current best tile: 512\n",
            "\n",
            "500 moves made\n",
            "Total Score: 9368\n",
            "Current best tile: 1024\n",
            "\n",
            "550 moves made\n",
            "Total Score: 9964\n",
            "Current best tile: 1024\n",
            "\n",
            "600 moves made\n",
            "Total Score: 10908\n",
            "Current best tile: 1024\n",
            "\n",
            "650 moves made\n",
            "Total Score: 11572\n",
            "Current best tile: 1024\n",
            "\n",
            "700 moves made\n",
            "Total Score: 12072\n",
            "Current best tile: 1024\n",
            "\n",
            "750 moves made\n",
            "Total Score: 13496\n",
            "Current best tile: 1024\n",
            "\n",
            "800 moves made\n",
            "Total Score: 14196\n",
            "Current best tile: 1024\n",
            "\n",
            "850 moves made\n",
            "Total Score: 15112\n",
            "Current best tile: 1024\n",
            "\n",
            "900 moves made\n",
            "Total Score: 15724\n",
            "Current best tile: 1024\n",
            "\n",
            "950 moves made\n",
            "Total Score: 20264\n",
            "Current best tile: 2048\n",
            "\n",
            "1000 moves made\n",
            "Total Score: 20888\n",
            "Current best tile: 2048\n",
            "\n",
            "1050 moves made\n",
            "Total Score: 21864\n",
            "Current best tile: 2048\n",
            "\n",
            "1100 moves made\n",
            "Total Score: 22324\n",
            "Current best tile: 2048\n",
            "\n",
            "1150 moves made\n",
            "Total Score: 22992\n",
            "Current best tile: 2048\n",
            "\n",
            "1200 moves made\n",
            "Total Score: 24444\n",
            "Current best tile: 2048\n",
            "\n",
            "1250 moves made\n",
            "Total Score: 25052\n",
            "Current best tile: 2048\n",
            "\n",
            "1300 moves made\n",
            "Total Score: 26008\n",
            "Current best tile: 2048\n",
            "\n",
            "1350 moves made\n",
            "Total Score: 26668\n",
            "Current best tile: 2048\n",
            "\n",
            "1400 moves made\n",
            "Total Score: 27132\n",
            "Current best tile: 2048\n",
            "\n",
            "Final Score: 27256\n",
            "Time to train: 1744.9484865665436\n",
            "Best tile achieved: 2048\n",
            "\n",
            "16 \t32 \t8 \t32\n",
            "8 \t256 \t64 \t2048\n",
            "4 \t512 \t128 \t4\n",
            "2 \t16 \t8 \t2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1744.9487600326538, 2048, 27256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK8i-haRG1pX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}